{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22711013",
   "metadata": {},
   "source": [
    "# M4 Agentic AI - Evaluation Methods in Research Reports\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "### 1.1. Lab overview\n",
    "\n",
    "In this lab, you will build on the workflow introduced in the graded lab at the end of **Module 3**.  \n",
    "The focus here is on how to integrate and apply **evaluation methods** inside a workflow that generates a short research report.  \n",
    "\n",
    "### 1.2. üéØ Learning outcomes\n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "* Write a function that can check the search results of a web search API for **preferred sources**.  \n",
    "* Create an evaluation to verify if your sources come from your **preferred domains**.  \n",
    "* Add a **component-level evaluation** to the `find_references` function.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc60d64f",
   "metadata": {},
   "source": [
    "## 2. Setup: Import libraries and load environment\n",
    "\n",
    "As in previous labs, you start by importing the required libraries and initializing your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9723175",
   "metadata": {
    "deletable": false,
    "editable": false,
    "height": 302
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Imports\n",
    "# =========================\n",
    "\n",
    "# --- Standard library \n",
    "from datetime import datetime\n",
    "import json\n",
    "import re\n",
    "\n",
    "# --- Third-party ---\n",
    "from aisuite import Client\n",
    "\n",
    "# --- Local / project ---\n",
    "import research_tools\n",
    "import utils\n",
    "\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1ac58",
   "metadata": {},
   "source": [
    "## 3. Steps ‚Äî Building your toolkit\n",
    "\n",
    "In this section, you will define the **steps** of the workflow. Each step plays a specific role:\n",
    "\n",
    "* **Step 1: find_references** ‚Äî gathers information  \n",
    "* **Step 2: write_draft** ‚Äî drafts the report  \n",
    "* **Step 3: reflect_and_rewrite** ‚Äî improves the draft  \n",
    "\n",
    "By combining these steps, you can build a workflow that turns a topic into a polished research report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fce954",
   "metadata": {},
   "source": [
    "### 3.1. Step 1: find_references\n",
    "\n",
    "As part of the workflow, you will now introduce the **research step**.  Its role is to **gather external information** using tools such as Arxiv, Tavily, and Wikipedia.  \n",
    "this function will search various websites for articles and other resources that are relevant to your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d88063",
   "metadata": {
    "height": 591
   },
   "outputs": [],
   "source": [
    "def find_references(task: str, model: str = \"openai:gpt-4o\", return_messages: bool = False):\n",
    "    \"\"\"Perform a research task using external tools (arxiv, tavily, wikipedia).\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a research function with access to:\n",
    "    - arxiv_tool: academic papers\n",
    "    - tavily_tool: general web search (return JSON when asked)\n",
    "    - wikipedia_tool: encyclopedic summaries\n",
    "\n",
    "    Task:\n",
    "    {task}\n",
    "\n",
    "    Today is {datetime.now().strftime('%Y-%m-%d')}.\n",
    "    \"\"\".strip()\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    tools = [\n",
    "        research_tools.arxiv_search_tool,\n",
    "        research_tools.tavily_search_tool,\n",
    "        research_tools.wikipedia_search_tool,\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",\n",
    "            max_turns=5,\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        return (content, messages) if return_messages else content\n",
    "    except Exception as e:\n",
    "        return f\"[Model Error: {e}]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ooflumaf0z",
   "metadata": {},
   "source": [
    "Run the following cell to try out the **research function**.  You will ask it to find two recent papers about neural networks on arXiv and then display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7sbp6250z5l",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "research_task = \"Find 2 recent papers about recent developments in black hole science\"\n",
    "research_result = find_references(research_task)\n",
    "\n",
    "utils.print_html(\n",
    "    research_result[:300] + \"...\" if len(research_result) > 300 else research_result,\n",
    "    title=\"Research Function Output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3fc5fd",
   "metadata": {},
   "source": [
    "### 3.2. Step 2: write_draft\n",
    "\n",
    "This step generates a structured academic or technical draft based on your research results.  \n",
    "Its role is to create a clear, organized first version of the report.  \n",
    "\n",
    "You will use this step whenever you need to transform your collected references into a coherent draft report.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068e7beb",
   "metadata": {
    "height": 319
   },
   "outputs": [],
   "source": [
    "def write_draft(task: str, model: str = \"openai:o4-mini\") -> str:\n",
    "    \"\"\"Generate a well-structured academic/technical draft based on the given task.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a writing function specialized in clear and well-structured academic/technical content.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": task},\n",
    "    ]\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=1.0\n",
    "    )\n",
    "    return resp.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fp71uica5",
   "metadata": {},
   "source": [
    "Run the following cell to try out the **writer function**.  You will ask it to create a short technical report about quantum computing, organized into four sections: Introduction, Key Principles, Applications, and Conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fxje1gxmx8",
   "metadata": {
    "height": 164
   },
   "outputs": [],
   "source": [
    "writing_task = \"Write a brief technical report about recent developments in black hole science: Introduction, Key Principles, Applications, Conclusion\"\n",
    "\n",
    "draft_result = write_draft(writing_task)\n",
    "\n",
    "utils.print_html(\n",
    "    draft_result[:400] + \"...\" if len(draft_result) > 400 else draft_result,\n",
    "    title=\"Writer Function Output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0037fd",
   "metadata": {},
   "source": [
    "### 3.3. Step 3: reflect_and_rewrite\n",
    "\n",
    "This step reviews the draft created by the `write_draft` step.  \n",
    "It evaluates the text for clarity, structure, and coherence, then applies improvements to produce a clearer and higher-quality report.  \n",
    "\n",
    "You will use this step whenever you need to refine a draft into its final polished version.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b389709",
   "metadata": {
    "height": 302
   },
   "outputs": [],
   "source": [
    "def reflect_and_rewrite(task: str, model: str = \"openai:o4-mini\") -> str:\n",
    "    \"\"\"Reflect on, critique, and improve a draft to produce a clearer, higher-quality version.\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a reflection and rewrite function. Reflect on, critique, and improve drafts.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": task},\n",
    "    ]\n",
    "    \n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=1\n",
    "    )\n",
    "    return resp.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ptr2zq2i4b",
   "metadata": {},
   "source": [
    "Run the following cell to try out the **reflection and rewrite function**.  You will provide it with a simple draft and ask it to improve clarity, structure, and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yewa8nmfwv9",
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "draft_to_edit = (\n",
    "    \"Black holes are very mysterious. They are heavy and strong. \"\n",
    "    \"Scientists study them because they are interesting. \"\n",
    "    \"Sometimes they are in movies. We don‚Äôt know everything about them.\"\n",
    ")\n",
    "\n",
    "editing_task = f\"Improve this draft: {draft_to_edit}\"\n",
    "edited_result = reflect_and_rewrite(editing_task)\n",
    "\n",
    "utils.print_html(\n",
    "    edited_result[:300] + \"...\" if len(edited_result) > 300 else edited_result,\n",
    "    title=\"Reflection & Rewrite Output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f095c",
   "metadata": {},
   "source": [
    "## 4. Evaluate the web domains returned by the research step\n",
    "\n",
    "In the video, Andrew explored the case where web search results were of poor quality.  \n",
    "Now, you‚Äôll create a **component-level evaluation** that counts how many of the web sources returned by the `find_references` step belong to your list of **preferred domains**.  \n",
    "\n",
    "For this evaluation case, you will focus on the topic *‚Äúrecent developments in black hole science‚Äù*, one of the examples shown in the course.  \n",
    "\n",
    "This evaluation will take the form of a single function that performs an **objective check** with a **per-example ground truth**.  \n",
    "Specifically, it will:\n",
    "\n",
    "- Parse the Tavily output (JSON string or list of dicts).  \n",
    "- Check how many URLs belong to a predefined allow-list of **preferred domains** (`TOP_DOMAINS`).  \n",
    "- Compute the ratio of preferred vs. total results.  \n",
    "- Return a boolean flag (**PASS/FAIL**) together with a detailed Markdown summary that can be directly included in reports.  \n",
    "\n",
    "<img src='M4-UGL-1.png' width=50%></img>\n",
    "\n",
    "<div style=\"border:1px solid #fca5a5; border-left:6px solid #ef4444; background:#fee2e2; border-radius:6px; padding:12px 14px; color:#7f1d1d; font-family:system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,sans-serif;\">\n",
    "  <strong>üîé Why this is an objective evaluation:</strong><br><br>\n",
    "  Each URL retrieved from Tavily is compared against a predefined allow-list of <em>preferred domains</em> (<code>TOP_DOMAINS</code>):<br>\n",
    "  ‚Ä¢ If the domain matches ‚Üí ‚úÖ PREFERRED<br>\n",
    "  ‚Ä¢ Otherwise ‚Üí ‚ùå NOT PREFERRED<br><br>\n",
    "  This yields a clear PASS/FAIL signal based on whether the ratio of preferred sources exceeds the threshold.  \n",
    "  Because the ground truth (preferred vs. not preferred) is explicitly defined per example, the evaluation is objective and reproducible.\n",
    "</div>\n",
    "\n",
    "In the next cell, you will find the definition of this function.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ccbbb7",
   "metadata": {
    "height": 1237
   },
   "outputs": [],
   "source": [
    "TOP_DOMAINS = {\n",
    "    # General reference / institutions / publishers\n",
    "    \"wikipedia.org\", \"nature.com\", \"science.org\", \"sciencemag.org\", \"cell.com\",\n",
    "    \"mit.edu\", \"stanford.edu\", \"harvard.edu\", \"nasa.gov\", \"noaa.gov\", \"europa.eu\",\n",
    "\n",
    "    # CS/AI venues & indexes\n",
    "    \"arxiv.org\", \"acm.org\", \"ieee.org\", \"neurips.cc\", \"icml.cc\", \"openreview.net\",\n",
    "\n",
    "    # Other reputable outlets\n",
    "    \"elifesciences.org\", \"pnas.org\", \"jmlr.org\", \"springer.com\", \"sciencedirect.com\",\n",
    "\n",
    "    # Extra domains (case-specific additions)\n",
    "    \"pbs.org\", \"nova.edu\", \"nvcc.edu\", \"cccco.edu\",\n",
    "\n",
    "    # Well known programming sites\n",
    "    \"codecademy.com\", \"datacamp.com\"\n",
    "}\n",
    "\n",
    "def evaluate_tavily_results(TOP_DOMAINS, raw: str, min_ratio=0.4):\n",
    "    \"\"\"\n",
    "    Evaluate whether plain-text research results mostly come from preferred domains.\n",
    "\n",
    "    Args:\n",
    "        TOP_DOMAINS (set[str]): Set of preferred domains (e.g., 'arxiv.org', 'nature.com').\n",
    "        raw (str): Plain text or Markdown containing URLs.\n",
    "        min_ratio (float): Minimum preferred ratio required to pass (e.g., 0.4 = 40%).\n",
    "\n",
    "    Returns:\n",
    "        tuple[bool, str]: (flag, markdown_report)\n",
    "            flag -> True if PASS, False if FAIL\n",
    "            markdown_report -> Markdown-formatted summary of the evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract URLs from the text\n",
    "    url_pattern = re.compile(r'https?://[^\\s\\]\\)>\\}]+', flags=re.IGNORECASE)\n",
    "    urls = url_pattern.findall(raw)\n",
    "\n",
    "    if not urls:\n",
    "        return False, \"\"\"### Evaluation ‚Äî Tavily Preferred Domains\n",
    "No URLs detected in the provided text. \n",
    "Please include links in your research results.\n",
    "\"\"\"\n",
    "\n",
    "    # Count preferred vs total\n",
    "    total = len(urls)\n",
    "    preferred_count = 0\n",
    "    details = []\n",
    "\n",
    "    for url in urls:\n",
    "        domain = url.split(\"/\")[2]\n",
    "        preferred = any(td in domain for td in TOP_DOMAINS)\n",
    "        if preferred:\n",
    "            preferred_count += 1\n",
    "        details.append(f\"- {url} ‚Üí {'‚úÖ PREFERRED' if preferred else '‚ùå NOT PREFERRED'}\")\n",
    "\n",
    "    ratio = preferred_count / total if total > 0 else 0.0\n",
    "    flag = ratio >= min_ratio\n",
    "\n",
    "    # Markdown report\n",
    "    report = f\"\"\"\n",
    "### Evaluation ‚Äî Tavily Preferred Domains\n",
    "- Total results: {total}\n",
    "- Preferred results: {preferred_count}\n",
    "- Ratio: {ratio:.2%}\n",
    "- Threshold: {min_ratio:.0%}\n",
    "- Status: {\"‚úÖ PASS\" if flag else \"‚ùå FAIL\"}\n",
    "\n",
    "**Details:**\n",
    "{chr(10).join(details)}\n",
    "\"\"\"\n",
    "    return flag, report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9pdatdlg4t",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid #93c5fd; border-left:6px solid #3b82f6; background:#dbeafe; border-radius:6px; padding:12px 14px; color:#1e3a8a; font-family:system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,sans-serif;\">\n",
    "Run the next cell to see a small sample of preferred domains and evaluate two URLs (one preferred, one not).\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4x867zzg5",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "utils.print_html(json.dumps(list(TOP_DOMAINS)[:4], indent=2), title=\"Sample Trusted Domains\")\n",
    "\n",
    "utils.print_html(\"<h3>Research Results</h3>\" + research_result, title=\"Research Results\")\n",
    "\n",
    "flag, report = evaluate_tavily_results(TOP_DOMAINS, research_result)\n",
    "utils.print_html(\"<pre>\" + report + \"</pre>\", title=\"<h3>Evaluation Summary</h3>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1b4e92",
   "metadata": {},
   "source": [
    "## 5. End-to-End: Run the Workflow\n",
    "\n",
    "In this final step, you will run the **full workflow** to generate a short research report. The process will:\n",
    "\n",
    "a) Use the **research function** to gather information from external sources (Tavily, Wikipedia, arXiv).  \n",
    "\n",
    "b) Run an **evaluation step** to check whether your sources come from preferred domains.  \n",
    "\n",
    "c) Pass the results to the **writer function** to create a first draft in Markdown.  \n",
    "\n",
    "d) (Optional) Improve the draft with the **reflection and rewrite function** for clarity and style.  \n",
    "\n",
    "By default, the evaluation uses a threshold of `min_ratio = 0.4` (40%). This means at least 40% of Tavily results must come from preferred `TOP_DOMAINS`. You can adjust this ratio if you want the evaluation to be stricter or more relaxed.\n",
    "\n",
    "You will now try it out with the topic **Ensemble Kalman filter** and follow the workflow end to end.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a90bbdd",
   "metadata": {
    "height": 404
   },
   "outputs": [],
   "source": [
    "# 1) Run research\n",
    "topic = \"recent developments in black hole science\"\n",
    "research_task = f\"Find 2‚Äì3 key papers and trusted overviews about {topic}.\"\n",
    "research_output = find_references(research_task)\n",
    "\n",
    "utils.print_html(research_output, title=f\"<h3>Research Results on {topic}</h3>\")\n",
    "\n",
    "# 2) Evaluate sources (Tavily references only)\n",
    "flag, eval_md = evaluate_tavily_results(TOP_DOMAINS, research_output, min_ratio=0.4)\n",
    "utils.print_html(\"<pre>\" + eval_md + \"</pre>\", title=\"<h3>Evaluation Summary</h3>\")\n",
    "\n",
    "# 3) Draft the report\n",
    "writing_task = (\n",
    "    f\"Write a concise Markdown research report about {topic} using ONLY these research results:\\n{research_output}\"\n",
    ")\n",
    "draft = write_draft(writing_task)\n",
    "utils.print_html(\"<pre>\" + draft + \"</pre>\", title=\"<h3>Draft Report</h3>\")\n",
    "\n",
    "# 4) Optional reflection & rewrite\n",
    "final_report = reflect_and_rewrite(f\"Improve this draft for clarity and structure (return Markdown only):\\n{draft}\")\n",
    "utils.print_html(\"<pre>\" + final_report + \"</pre>\", title=\"<h3>üßæ Reflection & Rewrite Report</h3>\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3e7205",
   "metadata": {},
   "source": [
    "## 6. Takeaways\n",
    "\n",
    "* **Evaluation is essential**: explicitly checking your sources against preferred domains ensures that your research is grounded in reliable references.  \n",
    "* **Confidence in results**: evaluation steps provide a clear PASS/FAIL signal, helping you decide whether to trust the output or refine your queries.  \n",
    "* **Graceful degradation**: even if the evaluation fails, the workflow continues‚Äîshowing that imperfect inputs can still be useful, but should be treated with caution.  \n",
    "* **Better reports**: combining research steps with explicit evaluation and refinement leads to concise, clear, and credible research reports.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927397e3",
   "metadata": {},
   "source": [
    "<div style=\"border:1px solid #22c55e; border-left:6px solid #16a34a; background:#dcfce7; border-radius:6px; padding:14px 16px; color:#064e3b; font-family:system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,sans-serif;\">\n",
    "\n",
    "üéâ <strong>Congratulations!</strong>\n",
    "\n",
    "You just built a research workflow that doesn‚Äôt stop at gathering information‚Äîit also <em>evaluates the quality of your sources</em>.  \n",
    "By checking results against preferred domains, you introduced a simple but powerful safeguard that makes your reports more credible.  \n",
    "\n",
    "You learned that component-level evaluation adds transparency and confidence: a clear PASS/FAIL signal tells you when to trust the output and when to refine your queries.  \n",
    "Even when evaluation fails, the workflow continues, showing that you can still produce results while keeping quality in mind.  \n",
    "\n",
    "With this skill, you‚Äôre ready to design <strong>workflows</strong> that not only generate content but also <strong>assess source reliability</strong>, ensuring your outputs are both useful and trustworthy. üåü\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
